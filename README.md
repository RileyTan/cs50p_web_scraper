# web scraper for tables
#### Video Demo:  https://youtu.be/hoKJ7FWhiek?si=kENyGwgPt2U_9fr5
#### Description:

_project.py_ is a python web scraper that is capable of scraping simple tables and converting them into csv files.

All files and folders are located in cs50p_final_project with project.py and test_project.py in the root.

Within a separate folder called cs50p_final_project/data_cleaning also contains a program that imports a csv file generated by _project.py_ and cleans up errors within. (Not demonstrated within my Youtube video. )


## Table of Contents

- [Data Collection](#data-collection)
- [Data Cleanup](#data-cleanup)

## Data Collection

_project.py_ contains the main project alongside 3 custom functions `pinpoint_table`, `save_html_as_dictionary`, and `save_to_csv`.


After the initial stages of using Requests to send HTTP requests to the desired page and transforming the entire HTML page into a Beautiful Soup object, the process of isolating the interested table, going through the data and saving the scraped data into a csv file is handled by `pinpoint_table`, `save_html_as_dictionary`, and `save_to_csv` respectively.

1. `pinpoint_table`(soup)

The whole chunk of code is embedded within an `if, else` conditional - users of the program are given the choice of inspecting the source code for the table's class or if its too difficult to look for the relevant information, they can match the table directly with the web page's content.

Should the user choose to inspect the source code, we ask for the table's class and just in case there are multiple tables sharing the same class, we ask for the index of the table too.

If the user chooses not to inspect the source code, we first use BeautifulSoup's _find_all()_ function to get all instances of `table`  and create a list of strings with the user's input. We initialise a variable called `max_match`  to track which table has the most number of matches and we settle with that.

2. `save_html_as_dictionary(interested_table)`

This function goes through the BeautifulSoup object returned by `pinpoint_table` to acquire a list of dictionaries.

Using `find_all` to get all `tr` within the interested_table. Looping through each `tr`, we nest another loop to go through the data within each `tr`. Each iteration of `tr` will yield us a dictionary that we append to a list called `table_data`.

3. `save_to_csv(results, filename)`

This function converts the list of dictionaries, ie the `results` that `save_html_as_dictionary` returns, into a csv file. Using `with open(filename, 'w') as file`, taking `headers` from the first row of `results`, we write each row of `results` using a loop into the csv file called `filename`.

## Data Cleanup

The file _links.txt_ tracks the urls of pages that work well with the web scraper and those that return a csv file that has faulty rows. _cleaning.py_ stored within the folder called _Data Cleaning_ works with pandas to fix faulty rows.

https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index is one page that yield a table with faulty rows when used with _project.py_.

There are 3 rounds of fixing that work on the same set of faulty rows yielded by `select_faulty`. The 3 rounds are named `fix_round1`, `fix_round2` and `fix_round3`.

Data masking is used to identify the rows that have a null or negative value in the Rank column and the `loc` accessor is used to modify the specified masked rows within the data frame and shift() is used to move the columns to its correct position.
